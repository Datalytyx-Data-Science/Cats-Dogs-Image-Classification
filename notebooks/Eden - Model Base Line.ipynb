{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Input Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pipeline that will pull data from the training arrays (in memory) into the model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = (dataset.shuffle(features.shape[0]) #Shuffle across the full number of examples\n",
    "               .repeat(epochs = 1)\n",
    "               .batch(batch_size))\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we define the graph. In this case we pull out several metrics at evaulation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, params['image_size'], params['image_size'], 3], name = \"input_layer\") #needs reshaping with new data\n",
    "    \n",
    "    #Convolutional Layer # 1\n",
    "    conv1 = tf.layers.conv3d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=params[\"kernel_size\"],    #Kernal size too small for data?\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.leaky_relu,\n",
    "        name = \"conv1\")   \n",
    "    \n",
    "    #Average activations of convolutional layer 1\n",
    "    with tf.variable_scope('Activations1'):\n",
    "        average_density_1 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv1 > 0), tf.float32), axis=[1]), name = \"average_density_1\")\n",
    "        tf.summary.scalar('AvergageDensity1', average_density_1)\n",
    "    \n",
    "    #Pooling Layers #1\n",
    "    pool1 = tf.layers.max_pooling3d(\n",
    "        inputs=conv1, \n",
    "        pool_size=2,\n",
    "        strides=2,\n",
    "        name = \"pool1\")\n",
    "    \n",
    "    #Convolutional Layer # 2\n",
    "    conv2 = tf.layers.conv3d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=params[\"kernel_size\"],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.leaky_relu,\n",
    "        name = \"conv2\")\n",
    "    \n",
    "    #Log the average activations of the second layer\n",
    "    with tf.variable_scope('Activations1'):\n",
    "        average_density_2 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv2 > 0), tf.float32), axis=[1]), name = \"average_density_2\")\n",
    "        tf.summary.scalar('AvergageDensity2', average_density_2)\n",
    "    \n",
    "    #Pooling layer # 2\n",
    "\n",
    "    pool2 = tf.layers.max_pooling3d(\n",
    "        inputs=conv2,\n",
    "        pool_size=2,\n",
    "        strides=2,\n",
    "        name = \"pool2\")\n",
    "    \n",
    "    #Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, int(pool2.shape[2]) * int(pool2.shape[1] * int(pool2.shape(3)))])\n",
    "    \n",
    "    dense = tf.layers.dense(\n",
    "        inputs=pool2_flat,\n",
    "        units=128,\n",
    "        activation=tf.nn.relu,\n",
    "        name = \"dense\") #RELU NOT LEAKY!!!\n",
    "\n",
    "\n",
    "        \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense,\n",
    "        rate=0.4,\n",
    "        training = mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    \n",
    "    #Logits layer\n",
    "    \n",
    "    logits = tf.layers.dense(\n",
    "        inputs=dropout,\n",
    "        units=2,\n",
    "        name = \"logits\")\n",
    "    \n",
    "#--------------- MODEL OUTPUT STRUCTURES ----------------\n",
    "\n",
    "    predictions = {\n",
    "\n",
    "        #Generate Predictions (for PREDICT and EVAL mode)\n",
    "        \n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \n",
    "        #Add 'softmax_tensor' to the  graph. It is used for the\n",
    "        #PREDICT by the 'logging_hook'\n",
    "        \n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        labelsOH = tf.one_hot(labels, 2)\n",
    "        correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(logits), 1), tf.argmax(labelsOH, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        #log the accuracy\n",
    "        tf.summary.scalar('training_accuracy', accuracy)\n",
    "\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    #Calculate Loss\n",
    "    \n",
    "    with tf.variable_scope('Loss_Layer'):\t\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    #Create a logging hook for training metrics\n",
    "\n",
    "    train_logging_hook = tf.train.SummarySaverHook(\n",
    "        save_steps = 50,\n",
    "        output_dir = params[\"dir\"],\n",
    "        summary_op = tf.summary.merge_all())\n",
    "\n",
    "\n",
    "\n",
    "#     #Load up the image maps for conv1 from a checkpoint of the sparse encoder\n",
    "#     tf.train.init_from_checkpoint(params[\"checkpoint\"],\n",
    "#                                   {'conv1/kernel':'conv1/kernel', #This overrides default initialization ops of specified variables.\n",
    "#                                     'conv1/bias':'conv1/bias'})\n",
    "    \n",
    "    #Configure the training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate']) #CHANGED FOR SPARSITY MODEL\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, \n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, \n",
    "            loss=loss, \n",
    "            train_op=train_op, \n",
    "            training_hooks = [train_logging_hook]  #ADDED LOGGING HOOK\n",
    "        ) \n",
    "    \n",
    "    # Add evaluation metric (for EVAL mode), These give final performance metrics.\n",
    "    \n",
    "    eval_metric_ops = {\n",
    "        \"final_accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"]), #Calculates how often the predictions matches the labels \n",
    "        \n",
    "        \"roc_auc_score\": tf.metrics.auc( \n",
    "            labels = labels, predictions = predictions[\"classes\"]), #Computes the approximate AUC via a Riemann sum\n",
    "        \n",
    "        \"sensativity\": tf.metrics.true_positives(\n",
    "            labels = labels, predictions = predictions[\"classes\"]), #Sum the weights of true-positives\n",
    "        \n",
    "        \"false-positive (1 - specificity)\": tf.metrics.false_positives(\n",
    "            labels = labels, predictions = predictions[\"classes\"]), #Sum the weights of false-positives\n",
    "        \n",
    "        \"precision\": tf.metrics.precision(\n",
    "            labels = labels, predictions = predictions[\"classes\"]) #Computes the precision of the predictions with respect to the labels.\n",
    "    }\n",
    "    \n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        loss = loss,\n",
    "        eval_metric_ops = eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add params image_size and learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
