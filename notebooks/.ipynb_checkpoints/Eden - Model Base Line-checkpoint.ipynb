{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Input Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pipeline that will pull data from the training arrays (in memory) into the model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = (dataset.shuffle(features.shape[0]) #Shuffle across the full number of examples\n",
    "               .repeat(epochs = 1)\n",
    "               .batch(batch_size))\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we define the graph. In this case we pull out several metrics at evaulation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, params['image_size'], params['image_size'], 3], name = \"input_layer\") #needs reshaping with new data\n",
    "\t\n",
    "\t#Convolutional Layer # 1\t\n",
    "\tconv1 = tf.layers.conv3d(\t\t\t\t\t\t\t\t\t\t\n",
    "\t\tinputs=input_layer,\n",
    "\t\tfilters=32,\t\t\t\t\t\t\n",
    "\t\tkernel_size=params[\"kernel_size\"],\t\t\t#Kernal size too small for data?\n",
    "\t\tpadding=\"same\",\n",
    "\t\tactivation=tf.nn.leaky_relu,\n",
    "\t\tname = \"conv1\")   \n",
    "\t\t\t \n",
    "\t#Average activations of convolutional layer 1\n",
    "\twith tf.variable_scope('Activations1'):\n",
    "\t\taverage_density_1 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv1 > 0), tf.float32), axis=[1]), name = \"average_density_1\")\n",
    "\t\ttf.summary.scalar('AvergageDensity1', average_density_1)\n",
    "\t\n",
    "\t#Pooling Layers #1\n",
    "\tpool1 = tf.layers.max_pooling3d(\n",
    "\t\tinputs=conv1,\t\t\t\t\t \n",
    "\t\tpool_size=2,\t\t\t\t\t\n",
    "\t\tstrides=2,\n",
    "\t\tname = \"pool1\")\n",
    "\t\n",
    "\t#Convolutional Layer # 2\n",
    "\tconv2 = tf.layers.conv3d(\n",
    "\t\tinputs=pool1,\t\t\n",
    "\t\tfilters=64,\n",
    "\t\tkernel_size=params[\"kernel_size\"],\n",
    "\t\tpadding=\"same\",\n",
    "\t\tactivation=tf.nn.leaky_relu,\n",
    "\t\tname = \"conv2\")\n",
    "\t\n",
    "\t#Log the average activations of the second layer\n",
    "\twith tf.variable_scope('Activations1'):\n",
    "\t\taverage_density_2 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv2 > 0), tf.float32), axis=[1]), name = \"average_density_2\")\n",
    "\t\ttf.summary.scalar('AvergageDensity2', average_density_2)\n",
    "\t\n",
    "\t#Pooling layer # 2\n",
    "\n",
    "\tpool2 = tf.layers.max_pooling3d(\n",
    "\t\tinputs=conv2,\n",
    "\t\tpool_size=2,\n",
    "\t\tstrides=2,\n",
    "\t\tname = \"pool2\")\n",
    "\t\n",
    "\t#Dense Layer\n",
    "\tpool2_flat = tf.reshape(pool2, [-1, int(pool2.shape[2]) * int(pool2.shape[1] * int(pool2.shape(3)))])\n",
    "\t\n",
    "\tdense = tf.layers.dense(\n",
    "\t\tinputs=pool2_flat,\n",
    "\t\tunits=128,\n",
    "\t\tactivation=tf.nn.relu,\n",
    "\t\tname = \"dense\") #RELU NOT LEAKY!!!\n",
    "\n",
    "\n",
    "\t\t\n",
    "\tdropout = tf.layers.dropout(\n",
    "\t\tinputs=dense,\n",
    "\t\trate=0.4,\n",
    "\t\ttraining = mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\t\n",
    "\t\n",
    "\t#Logits layer\n",
    "\t\n",
    "\tlogits = tf.layers.dense(\n",
    "\t\tinputs=dropout,\n",
    "\t\tunits=2,\n",
    "\t\tname = \"logits\")\n",
    "\t\n",
    "#--------------- MODEL OUTPUT STRUCTURES ----------------\n",
    "\n",
    "\tpredictions = {\n",
    "\n",
    "\t\t#Generate Predictions (for PREDICT and EVAL mode)\n",
    "\t\t\n",
    "\t\t\"classes\": tf.argmax(input=logits, axis=1),\n",
    "\t\t\n",
    "\t\t#Add 'softmax_tensor' to the  graph. It is used for the\n",
    "\t\t#PREDICT by the 'logging_hook'\n",
    "\t\t\n",
    "\t\t\"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "\t}\n",
    "\t\n",
    "\n",
    "\twith tf.variable_scope('Accuracy'):\n",
    "\t\tlabelsOH = tf.one_hot(labels, 2)\n",
    "\t\tcorrect_prediction = tf.equal(tf.argmax(tf.nn.softmax(logits), 1), tf.argmax(labelsOH, 1))\n",
    "\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\t\t\n",
    "\t\t#log the accuracy\n",
    "\t\ttf.summary.scalar('training_accuracy', accuracy)\n",
    "\n",
    "\t\n",
    "\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\t\n",
    "\t#Calculate Loss\n",
    "\t\n",
    "\twith tf.variable_scope('Loss_Layer'):\t\n",
    "\t\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\t\n",
    "\t#Create a logging hook for training metrics\n",
    "\n",
    "\ttrain_logging_hook = tf.train.SummarySaverHook(\n",
    "\t\tsave_steps = 50,\n",
    "\t\toutput_dir = params[\"dir\"],\n",
    "\t\tsummary_op = tf.summary.merge_all())\n",
    "\n",
    "\t\n",
    "# \t#Load up the image maps for conv1 from a checkpoint of the sparse encoder\n",
    "# \ttf.train.init_from_checkpoint(params[\"checkpoint\"],\n",
    "# \t\t\t\t\t\t\t\t{'conv1/kernel':'conv1/kernel', #This overrides default initialization ops of specified variables.\n",
    "# \t\t\t\t\t\t\t\t'conv1/bias':'conv1/bias'})\n",
    "\t\n",
    "\t#Configure the training Op (for TRAIN mode)\n",
    "\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\n",
    "\t\toptimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate']) #CHANGED FOR SPARSITY MODEL\n",
    "\t\ttrain_op = optimizer.minimize(\n",
    "\t\t\tloss=loss, \n",
    "\t\t\tglobal_step=tf.train.get_global_step())\n",
    "\t\treturn tf.estimator.EstimatorSpec(\n",
    "\t\t\tmode=mode, \n",
    "\t\t\tloss=loss, \n",
    "\t\t\ttrain_op=train_op, \n",
    "\t\t\ttraining_hooks = [train_logging_hook]  #ADDED LOGGING HOOK\n",
    "        ) \n",
    "\t\n",
    "\t# Add evaluation metric (for EVAL mode), These give final performance metrics.\n",
    "\t\n",
    "\teval_metric_ops = {\n",
    "\t\t\"final_accuracy\": tf.metrics.accuracy(\n",
    "\t\t\tlabels=labels, predictions=predictions[\"classes\"]), #Calculates how often the predictions matches the labels \n",
    "\t\t\n",
    "\t\t\"roc_auc_score\": tf.metrics.auc( \n",
    "\t\t\tlabels = labels, predictions = predictions[\"classes\"]), #Computes the approximate AUC via a Riemann sum\n",
    "\t\t\n",
    "\t\t\"sensativity\": tf.metrics.true_positives(\n",
    "\t\t\tlabels = labels, predictions = predictions[\"classes\"]), #Sum the weights of true-positives\n",
    "\t\t\n",
    "\t\t\"false-positive (1 - specificity)\": tf.metrics.false_positives(\n",
    "\t\t\tlabels = labels, predictions = predictions[\"classes\"]), #Sum the weights of false-positives\n",
    "\t\t\n",
    "\t\t\"precision\": tf.metrics.precision(\n",
    "\t\t\tlabels = labels, predictions = predictions[\"classes\"]) #Computes the precision of the predictions with respect to the labels.\n",
    "\t}\n",
    "\t\n",
    "\n",
    "\treturn tf.estimator.EstimatorSpec(\n",
    "\t\tmode = mode,\n",
    "        loss = loss,\n",
    "        eval_metric_ops = eval_metric_ops\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add params image_size and learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
